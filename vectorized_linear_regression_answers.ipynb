{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorized-linear-regression-answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized linear regression\n",
        "\n",
        "In this notebook, we will go through how to implement linear regression in the vectorized approach using numpy, which is the recommended way to do it in python.\n",
        "\n",
        "I strongly recommend spending about 10-15 minutes learning numpy before proceeding with this notebook. Moreover, it will be pretty important to know how to do matrix multiplication and taking the transpose of a matrix to implement the vectorized approaches."
      ],
      "metadata": {
        "id": "IUyNZTbgUvji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "import matplotlib.pyplot as plt # for plotting"
      ],
      "metadata": {
        "id": "qjnIkGtZVAFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing the hypothesis function\n",
        "Let's say you are given a data set of house areas ($\\text{km}^2$) in one column, and house prices (in \\$100,000) in the other column. Your goal is to find a hypothesis function where the hypothesis given by the linear model\n",
        "$$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1$$\n",
        "\n",
        "Let's organize our inputs in a matrix $X$ with the following contents:\n",
        "$$X = \\begin{bmatrix}\n",
        " x_{0}\\\\\n",
        " x_{1}\\\\\n",
        " \\vdots\\\\\n",
        " x_{m}\\\\\n",
        " \\end{bmatrix}$$\n",
        "\n",
        " and a matrix y representing the expected target for each input\n",
        "$$y = \\begin{bmatrix}\n",
        " y_{0}\\\\\n",
        " y_{1}\\\\\n",
        " \\vdots\\\\\n",
        " y_{m}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        " and a matrix $\\theta$ representing the parameters of our linear function\n",
        "$$\\theta = \\begin{bmatrix}\n",
        " \\theta_{0}\\\\\n",
        " \\theta_{1}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "How can we $h_\\theta(x)$ as a single matrix operation? \\\\\n",
        "First, for convenience, let's add a column of ones to the left of our input matrix $X$:\n",
        "$$X = \\begin{bmatrix}\n",
        " x^{(0)T}\\\\\n",
        " x^{(1)T}\\\\\n",
        " \\vdots\\\\\n",
        " x^{(m)T}\\\\\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        " 1 & x_{0}\\\\\n",
        " 1 & x_{1}\\\\\n",
        " \\vdots & \\vdots\\\\\n",
        " 1 & x_{m}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "In reality, $x^{(i)}= \\begin{bmatrix} x^{i}_{0} \\\\ x^{i}_{1}\\\\ \\end{bmatrix}$ but since we only have 1 input feature (house area) we can simply not worry about that (though this indexing notation will become very necessary when we move on to multi-feature linear regression). For now, let's not worry about that and simply present $x^{(i)}= \\begin{bmatrix} 1 \\\\ x_{i}\\\\ \\end{bmatrix}$ for simplicity. Note that by taking the [`transpose`](https://en.wikipedia.org/wiki/Transpose) of the matrix $x^{(i)}$ we get $x^{(i)T} =\\begin{bmatrix} 1 & x_{i}\\\\ \\end{bmatrix}$.\n",
        "\n",
        "\n",
        "Then, if we take the matrix product between $X$ and $\\theta$\n",
        "$$X\\theta = \n",
        "\\begin{bmatrix}\n",
        " 1 & x_{0}\\\\\n",
        " 1 & x_{1}\\\\\n",
        " \\vdots & \\vdots\\\\\n",
        " 1 & x_{m}\\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        " \\theta_{0}\\\\\n",
        " \\theta_{1}\\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        " \\theta_0 + \\theta_1x_{0}\\\\\n",
        " \\theta_0 + \\theta_1x_{1}\\\\\n",
        " \\vdots \\\\\n",
        " \\theta_0 + \\theta_1x_{m}\\\\\n",
        "\\end{bmatrix} $$ \n",
        "Which represents a vector `(a matrix of size (1, m))` corresponding to the predicted value for each input. This was all done in a single operation! Also, you will need to know [`matrix multiplication`](https://www.mathsisfun.com/algebra/matrix-multiplying.html) in order to understand all previous steps.\n",
        "\n",
        "The following may also be helpful.\n",
        "By taking the [`transpose`](https://en.wikipedia.org/wiki/Transpose) of the matrix $\\theta$ to get \n",
        "\n",
        "$$\\theta^T = \\begin{bmatrix}\n",
        " \\theta_{0}\n",
        " & \\theta_{1}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "I'll leave it to the reader to verify that $h_\\theta(x)=\\theta_Tx$."
      ],
      "metadata": {
        "id": "66Gu4f2DVK8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data\n",
        "To make things simple, the data we will produce is somewhat random but follows a trend line."
      ],
      "metadata": {
        "id": "ptrIM2qlg5dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.arange(10)\n",
        "y = X + (np.random.random(10) * 2 - 1)\n",
        "\n",
        "m = y.size  # number of training examples\n",
        "\n",
        "# adding a column of ones to the left of X\n",
        "X = np.stack([np.ones(m), X], axis=1)\n",
        "\n",
        "# initializing theta\n",
        "theta = np.zeros(2)"
      ],
      "metadata": {
        "id": "uQ8F-ngUhJTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorized cost function\n",
        "Using what we just put forward, try to implement a vectorized version of the cost function \n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$"
      ],
      "metadata": {
        "id": "4jkiM-jogJal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Tip: in order to take the matrix multiplication between two numpy arrays A and B, we do the following: A @ B`"
      ],
      "metadata": {
        "id": "vFnn57VirDG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(theta, X, y):\n",
        "  ''' \n",
        "  Compute cost for linear regression. Computes the cost of using theta as the\n",
        "  parameter for linear regression to fit the data points in X and y.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  theta is of shape (2,)\n",
        "  X is of shape (m, 2)\n",
        "  y is of shape (m,)\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  J, the value of the cost function\n",
        "  '''\n",
        "  m = y.size  # number of training examples\n",
        "    \n",
        "  # You need to return the following variables correctly\n",
        "  J = 0\n",
        "    \n",
        "  # ====================== YOUR CODE HERE =====================\n",
        "  error = X @ theta  - y\n",
        "  J = 1 / (2 * m) * error.T @ error\n",
        "  # ===========================================================\n",
        "  return J\n",
        "  \n"
      ],
      "metadata": {
        "id": "0udu1fwNaXS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your function here\n",
        "print(f\"The current cost is: {cost_function(theta, X, y)}\")"
      ],
      "metadata": {
        "id": "PWFObiU4p0SG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867be0cb-4293-4a5b-d275-0dbc3d5cf627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current cost is: 13.834660101129234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized gradient descent\n",
        "\n",
        "Recall that our goal is to minimize our cost function. The algorithsm we used for that is gradient descent, which is summarized as follows\n",
        "\n",
        "$\\text{Repeat until convergence}$ {\n",
        "$$ \\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)$$\n",
        "\n",
        "$$ \\theta_1 = \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_{i}$$\n",
        "}\n",
        "\n",
        "Where with each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
      ],
      "metadata": {
        "id": "1zq_yntFjsl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Tip: in order to take the transpose of a matrix A in numpy we do the following: A.T`"
      ],
      "metadata": {
        "id": "JWBaHo75o7E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = y.shape[0]  # number of training examples\n",
        "iterations = 2000\n",
        "alpha = 1e-2 # learning rate\n",
        "\n",
        "# Batch gradient descent\n",
        "for i in range(iterations):\n",
        "    # ==================== YOUR CODE HERE =================================\n",
        "    error = X @ theta  - y\n",
        "    delta = 1 / m * (X.T @ error)\n",
        "    theta -= alpha * delta\n",
        "\n",
        "    # =====================================================================\n"
      ],
      "metadata": {
        "id": "gDCAF0gMjsMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the results"
      ],
      "metadata": {
        "id": "yxBBct4orlZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the data points\n",
        "plt.plot(X[:, 1:], y, \"ro\")\n",
        "plt.xlabel(\"Area in km^2\")\n",
        "plt.ylabel(\"Cost in $100,000\")\n",
        "\n",
        "# plotting our line of best fit\n",
        "x_axis = np.linspace(0, 10, 100)\n",
        "x = np.stack([np.ones(x_axis.shape[0]), x_axis], axis=1)\n",
        "plt.plot(x_axis, x @ theta)"
      ],
      "metadata": {
        "id": "FBnt-NbgPrjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}